# 휴학하고 진행하는 연구 
>>방학동안 진행했던 강화학습 공부를 바탕으로 실습을 적용해보는 과정을 해보기로 하였다

이번 연구 목표는 클라우드 환경에서 딥러닝 중 강화학습의 모델을 학습시키는 과정을 비용과 시간적인 측면에서 효울적인 방법을 찾는 것이다.

1. 클라우드 환경의 CPU , GPU, TPU 에서 실행시켜 볼 수 있는 대중적인 강화학습 알고리즘을 찾기

>> (tutorial 폴더에) 제일 대중적인 예시로 쓰였던 cartpole code를 찾아보았다. 처음에는 dqn 방법을 이용한 코드를 찾아 돌려보았지만 
실행시켜보는 과정에서 dqn은 게임화면을 입력으로 받아와 학습하는 CNN을 이용하는 경우가 많다. 하지만 내가 사용하는 환경은 클라우드, 서버여서 display를 사용할 수 없다
따라서 찾은 코드 중에서 display 와 관련된 render() 함수가 없는 , display값으로 학습을 하지않는 다른 알고리즘 방법을 생각해본다.

>>보통 찾은 예시는 cpu 에서 실행해보는 코드이므로 gpu에서 사용가능한, cuda를 사용하는 코드로 찾아본다.

>> dqn은 같은 코드라도 돌릴때마다 학습이 잘되거나 안되거나 결과가 항상 학습이 잘되는 쪽이아닌 불안정한 상태였다. dqn은 cnn 방법을 이용하는 경우가 많고 그럴경우
내가 사용하는 환경 클라우드에서는 돌리기 어렵다는 결론이 나온다.또한 dqn은 replaymemory방법으로 sample들간의 상관관계를 해결하는데, replaymemory의 용량이 
너무 커서 느린 학습속도를 보이며 가치함수에 대한 (policy based) greedy policy이기때문에 불안정한 학습과정을 가져온다. 

>>따라서 DQN방법은 포기하고 A3C rl 알고리즘을 이용해보도록 한다. A3C는 policy optimization 방법이라서 DQN보다 안정적인 학습결과를 가져온다. 
A3C는 방학때 들어보았던 알고리즘이 아니라서 처음부터,, 다시 알아본다ㅜㅜ어렵다ㅜㅜㅜ.. A3C는 parallel training 에 초점을 맞춘 policy gradient method 이다
Critic 모델은 여러개의 Actor 모델이 병렬적으로 학습이 되면서 global parameter을 통해 시간 단위로 동기화되는 동안 value function을 학습한다. 
A3C 방법에 대해 자세히 알기 위해 논문 'Asynchronous Methods for Deep Reinforcement Learning'을 읽어본다
